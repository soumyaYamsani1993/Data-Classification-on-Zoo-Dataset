library(tree)

#To load the dataset
# Please use header=F
zoo=read.csv("/Users/ashwinibhoomi/Desktop/SEM-3/Data Mining/Project/zoo.data",header=F)

#Variable headers
names(zoo)=c("animal", "hair", "feathers", "eggs", "milk", "airborne","aquatic", "predator", "toothed", "backbone", "breathes", "venomous",
"fins", "legs", "tail", "domestic", "size", "type")

#To attach dataset
attach(zoo)

#To check missing values
zoo=na.omit(zoo)

#Deleting duplicate data
zoo=zoo[-c(26),]
rownames(zoo)=NULL


#Dropping the column animal name
zoo$animal=NULL

#Discretization of class label 
response=ifelse(zoo$type<=3,"No","Yes")
zoo=data.frame(zoo,response)
zoo=zoo[,-17]

#To check row and column dimensions
dim(zoo)

#------------------------Hold-out Method------------------------------------
#To create a decision tree with response as the class label based on all other attributes
tree.zoo=tree(response~.,zoo)

#Summary of the created tree
summary(tree.zoo)

#To display the tree structures and node labels
plot(tree.zoo)
text(tree.zoo,pretty=0)
tree.zoo

#Testing the model using predict function
tree.pred=predict(tree.zoo,type="class")
tree.pred

#Confusion matrix
table(tree.pred,response)

#Correct prediction rate
mean(tree.pred==response)

#Error prediction rate
mean(tree.pred!=response)

#Random select a sample of 60 observations of the data set as a training set and the rest 
#of the data set as a test set
set.seed(123)
train=sample(1:nrow(zoo), 60)
zoo.train=zoo[train,]
zoo.test=zoo[-train,]
response.train=response[train]
response.test=response[-train]
tree.zoo=tree(response~.,zoo.train)

#Train error
tree.pred=predict(tree.zoo,zoo.train,type="class")
table(tree.pred,response.train)
mean(tree.pred!=response.train)

#Test error
tree.pred=predict(tree.zoo,zoo.test,type="class")
table(tree.pred,response.test)
mean(tree.pred!=response.test)

#Cross validation to understand optimal level of tree complexity
set.seed(123)
cv.zoo=cv.tree(tree.zoo,FUN=prune.misclass)
cv.zoo

plot(cv.zoo$size ,cv.zoo$dev ,type="b")


#Check the tree with size 4
set.seed(123)
prune.zoo=prune.misclass(tree.zoo,best=4)
plot(prune.zoo)
text(prune.zoo,pretty=0)
tree.pred=predict(prune.zoo,zoo.test,type="class")
table(tree.pred,response.test)
mean(tree.pred!=response.test)

#Check the tree with size 3
set.seed(123)
prune.zoo=prune.misclass(tree.zoo,best=3)
plot(prune.zoo)
text(prune.zoo,pretty=0)
tree.pred=predict(prune.zoo,zoo.test,type="class")
table(tree.pred,response.test)
mean(tree.pred!=response.test)

#Check the tree with size 2
set.seed(123)
prune.zoo=prune.misclass(tree.zoo,best=2)
plot(prune.zoo)
text(prune.zoo,pretty=0)
tree.pred=predict(prune.zoo,zoo.test,type="class")
table(tree.pred,response.test)
mean(tree.pred!=response.test)


#------------------------Bagging Method------------------------------------

##Decision tree using bagging

#Package includes randomForest() to perform both bagging and random forest
library(randomForest)

#bagging - special case of a random forest with m = p
#ntree indicates the number of trees are generated by bagging
#mtry  indicates the number of variables are used at each split.

#Check with 50 trees
set.seed(123)
tree.zoo=randomForest(response~.,zoo.train, ntree=30,mtry=7)
tree.pred=predict(tree.zoo,zoo.test,type="class")
table(tree.pred,response.test)
mean(tree.pred!=response.test)

#Check with 20 trees
set.seed(123)
tree.zoo=randomForest(response~.,zoo.train, ntree=10,mtry=7)
tree.pred=predict(tree.zoo,zoo.test,type="class")
table(tree.pred,response.test)
mean(tree.pred!=response.test)


#Check with 10 trees
set.seed(123)
tree.zoo=randomForest(response~.,zoo.train, ntree=8,mtry=7)
tree.pred=predict(tree.zoo,zoo.test,type="class")
table(tree.pred,response.test)
mean(tree.pred!=response.test)

#------------------------RandomForest Method------------------------------------
#Decision tree using RandomForest
#By default, randomForest() uses about sqrt(p) variables when building a random forest of classification trees. sqrt(16)=4

#mtry=4
set.seed(123)
tree.zoo=randomForest(response~.,zoo.train, ntree=50, mtry=4)
tree.pred=predict(tree.zoo,zoo.test,type="class")
table(tree.pred,response.test)
mean(tree.pred!=response.test)

#mtry=5
set.seed(123)
tree.zoo=randomForest(response~.,zoo.train, ntree=50, mtry=5)
tree.pred=predict(tree.zoo,zoo.test,type="class")
table(tree.pred,response.test)
mean(tree.pred!=response.test)

#mtry=3
set.seed(123)
tree.zoo=randomForest(response~.,zoo.train, ntree=50, mtry=3)
tree.pred=predict(tree.zoo,zoo.test,type="class")
table(tree.pred,response.test)
mean(tree.pred!=response.test)

#------------------------Boosting Method------------------------------------
##Decision tree using boosting
library(tree)
library(gbm)
zoo=read.csv("/Users/ashwinibhoomi/Desktop/SEM-3/Data Mining/Project/zoo.data",header=F)
names(zoo)=c("animal", "hair", "feathers", "eggs", "milk", "airborne","aquatic", "predator", "toothed", "backbone", "breathes", "venomous",
"fins", "legs", "tail", "domestic", "size", "type")

attach(zoo)

zoo=na.omit(zoo)

#Deleting duplicate data
zoo=zoo[-c(26),]
rownames(zoo)=NULL

#Dropping the column animal name
zoo$animal=NULL

#Discretization of class label 
class.label=ifelse(zoo$type<=3,"No","Yes")
zoo=data.frame(zoo,class.label)
zoo=zoo[,-17]

#For binary classification, the response variable should be 0 or 1 if using Bernoulli distribution.
zoo$class.label=ifelse(zoo$class.label=="Yes",1,0)
zoo$class.label

set.seed(123)
train=sample(1:nrow(zoo),60)
zoo.train=zoo[train,]
zoo.test=zoo[-train,]
class.label.test=class.label[-train]

#Check with 40 trees, n.trees =no. of trees
set.seed(123)
tree.zoo=gbm(class.label~., zoo.train, distribution="bernoulli",n.trees=40)
tree.pred.prob=predict(tree.zoo, zoo.test, n.trees=40, type="response")
tree.pred=ifelse(tree.pred.prob>0.5, "Yes", "No")
table(class.label.test, tree.pred)
mean(tree.pred!=class.label.test)

#Check with 15 trees
set.seed(123)
tree.zoo=gbm(class.label~., zoo.train, distribution="bernoulli",n.trees=15)
tree.pred.prob=predict(tree.zoo, zoo.test, n.trees=15, type="response")
tree.pred=ifelse(tree.pred.prob>0.5, "Yes", "No")
table(class.label.test, tree.pred)
mean(tree.pred!=class.label.test)

#Check with 10 trees
set.seed(123)
tree.zoo=gbm(class.label~., zoo.train, distribution="bernoulli",n.trees=10)
tree.pred.prob=predict(tree.zoo, zoo.test, n.trees=10, type="response")
tree.pred=ifelse(tree.pred.prob>0.5, "Yes", "No")
table(class.label.test, tree.pred)
mean(tree.pred!=class.label.test)

#------------------------Naïve Bayes Method------------------------------------
##Naïve Bayes classifier
#The e1071 library contains implementations for Naive Bayes classification and Support Vector Machine. 
library(e1071)

zoo=read.csv("/Users/ashwinibhoomi/Desktop/SEM-3/Data Mining/Project/zoo.data",header=F)
names(zoo)=c("animal", "hair", "feathers", "eggs", "milk", "airborne","aquatic", "predator", "toothed", "backbone", "breathes", "venomous",
"fins", "legs", "tail", "domestic", "size", "type")

zoo=zoo[-c(26),]
rownames(zoo)=NULL
zoo$animal=NULL

response=ifelse(zoo$type<=3,"No","Yes")
zoo=data.frame(zoo,response)
zoo=zoo[,-17]

dim(zoo)
attach(zoo)

#Fitting the Naive Bayes model
Naive_Bayes_Model=naiveBayes(response~., zoo)

#Understanding the model summary
Naive_Bayes_Model

#Predicting dataset
NB_Predictions=predict(Naive_Bayes_Model,zoo)

#Confusion matrix for accuracy
table(NB_Predictions,response)
mean(NB_Predictions!=response)

#Train and test set
set.seed(123)
train=sample(1:nrow(zoo),70)
trainSet=zoo[train,]
testSet=zoo[-train,]
test.label=response[-train]
NB_2=naiveBayes(response~.,trainSet)
NB_Predictions_2=predict(NB_2,testSet)
table(NB_Predictions_2,test.label)
mean(NB_Predictions_2!=test.label)

#------------------------SVM-Linear Method------------------------------------
##Support vector machine using liner kernel with different costs
library(e1071)

zoo=read.csv("/Users/ashwinibhoomi/Desktop/SEM-3/Data Mining/Project/zoo.data",header=F)
names(zoo)=c("animal", "hair", "feathers", "eggs", "milk", "airborne","aquatic", "predator", "toothed", "backbone", "breathes", "venomous",
"fins", "legs", "tail", "domestic", "size", "type")

zoo=zoo[-c(26),]
rownames(zoo)=NULL
zoo$animal=NULL

response=ifelse(zoo$type<=3,"No","Yes")
zoo=data.frame(zoo,response)
zoo=zoo[,-17]

train=sample(1:nrow(zoo), 60)
zoo.train=zoo[train,]
zoo.test=zoo[-train,]

response.train=response[train]
response.test=response[-train]

#Fitting the model
svmfit=svm(response~.,data=zoo.train,kernel="linear",cost=0.01)
summary(svmfit)

#Training error rate
svm1.pred=predict(svmfit,newdata=zoo.train)
table(svm1.pred,response.train)
mean(svm1.pred!=response.train)

#Testing error rate
svm1.pred=predict(svmfit,newdata=zoo.test)
table(svm1.pred,response.test)
mean(svm1.pred!=response.test)


#Using tune() for cross validation
set.seed(123)
tune.out=tune(svm, response~., data=zoo, kernel="linear",ranges=list(cost=c(0.01,0.1,1,10,100)))
summary(tune.out)

bestmod=tune.out$best.model
summary(bestmod)

#To find the training error
pred=predict(tune.out$best.model, newdata=zoo.train)
table(response.train, pred)
mean(pred!=response.train)


#To find the testing error
pred=predict(tune.out$best.model, newdata=zoo.test)
table(response.test, pred)
mean(pred!=response.test)

#------------------------SVM-Radial Method--------------------------------
#Support Vector Machine with radial kernel and default gamma before tune-out
set.seed(123)

svmfit=svm(response~.,data=zoo.train,kernel="radial",gamma=1,cost=0.01)
summary(svmfit)

svm1.pred=predict(svmfit,newdata=zoo.train)
table(svm1.pred,response.train)
mean(svm1.pred!=response.train)

svm2.pred = predict(svmfit,newdata=zoo.test)
table(svm2.pred,response.test)
mean(svm2.pred!=response.test)

##different cost and gammas
tune.out=tune(svm, response~., data=zoo.train, kernel="radial",ranges=list(cost=c(0.001,0.01,0.1,1,10), gamma=c(1,2,3,4,5)))
summary(tune.out)

bestmod=tune.out$best.model
summary(bestmod)


#Test error
pred=predict(tune.out$best.model, newdata=zoo.test)
table(response.test, pred)
mean(pred!=response.test)

#------------------------SVM-Polynomial Method------------------------------------
#Support Vector Machine with polynomial kernel and default gamma before tune-out
set.seed(123)

svmfit=svm(response~.,data=zoo.train,kernel="polynomial",degree=2,cost=0.01)
summary(svmfit) 

svm1.pred=predict(svmfit,newdata=zoo.train)
table(svm1.pred,response.train)
mean(svm1.pred!=response.train)

svm2.pred = predict(svmfit,newdata=zoo.test)
table(svm2.pred,response.test)
mean(svm2.pred!=response.test)

##different cost and gammas
tune.out=tune(svm, response~., data=zoo.train, kernel="polynomial",degree=2,ranges=list(cost=c(0.001,0.01,0.1,1,10), gamma=c(0.2,0.5,1,2,3)))
summary(tune.out)


#Test error
pred=predict(tune.out$best.model, newdata=zoo.test)
table(response.test, pred)
mean(pred!=response.test)